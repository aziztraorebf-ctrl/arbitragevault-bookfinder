# Phase 7.0 - AutoSourcing Safeguards Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Protect production from token exhaustion by implementing hard limits, cost estimation, deduplication, and timeout protection for AutoSourcing jobs.

**Architecture:** Backend validation layer intercepts job submissions, estimates token cost, enforces limits (200 tokens, 120s timeout), deduplicates ASINs, then frontend displays cost estimate before submission.

**Tech Stack:** FastAPI, Pydantic validation, asyncio timeout, Pytest, Playwright E2E

**Prerequisites:** Phase 6 complete (28/28 tests passing), balance 1200+ tokens

**Token Cost:** ~50 tokens for testing (7 real API calls in tests)

---

## Task 1: Backend Constants and Schemas

**Goal:** Define protection limits and validation schemas

**Files:**
- Create: `backend/app/schemas/autosourcing_safeguards.py`
- Modify: `backend/app/api/v1/routers/autosourcing.py:25-42` (add import)

### Step 1.1: Write schema tests

Create: `backend/tests/schemas/test_autosourcing_safeguards_schemas.py`

```python
"""
Tests for AutoSourcing safeguards schemas.
"""
import pytest
from pydantic import ValidationError
from app.schemas.autosourcing_safeguards import (
    JobValidationResult,
    CostEstimateRequest,
    CostEstimateResponse,
    MAX_TOKENS_PER_JOB,
    MAX_PRODUCTS_PER_SEARCH,
    TIMEOUT_PER_JOB,
    MIN_TOKEN_BALANCE_REQUIRED
)


def test_constants_have_correct_values():
    """Verify safeguard constants are set to production values."""
    assert MAX_TOKENS_PER_JOB == 200
    assert MAX_PRODUCTS_PER_SEARCH == 10
    assert TIMEOUT_PER_JOB == 120
    assert MIN_TOKEN_BALANCE_REQUIRED == 50


def test_job_validation_result_creation():
    """Test JobValidationResult schema validation."""
    result = JobValidationResult(
        estimated_tokens=150,
        current_balance=1000,
        safe_to_proceed=True,
        warning_message=None
    )
    assert result.estimated_tokens == 150
    assert result.current_balance == 1000
    assert result.safe_to_proceed is True


def test_job_validation_result_with_warning():
    """Test JobValidationResult with warning message."""
    result = JobValidationResult(
        estimated_tokens=250,
        current_balance=1000,
        safe_to_proceed=False,
        warning_message="Exceeds MAX_TOKENS_PER_JOB (200)"
    )
    assert result.safe_to_proceed is False
    assert "Exceeds" in result.warning_message


def test_cost_estimate_request_validation():
    """Test CostEstimateRequest schema accepts discovery config."""
    request = CostEstimateRequest(
        discovery_config={
            "categories": ["books"],
            "max_results": 50
        }
    )
    assert "categories" in request.discovery_config
    assert request.discovery_config["max_results"] == 50


def test_cost_estimate_response_structure():
    """Test CostEstimateResponse with all fields."""
    response = CostEstimateResponse(
        estimated_tokens=75,
        current_balance=500,
        safe_to_proceed=True,
        warning_message=None,
        max_allowed=200,
        suggestion=None
    )
    assert response.estimated_tokens == 75
    assert response.max_allowed == 200
```

### Step 1.2: Run tests to verify they fail

Run:
```bash
cd backend
pytest tests/schemas/test_autosourcing_safeguards_schemas.py -v
```

Expected: FAIL with "ModuleNotFoundError: app.schemas.autosourcing_safeguards"

### Step 1.3: Create schema file

Create: `backend/app/schemas/autosourcing_safeguards.py`

```python
"""
Schemas for AutoSourcing safeguards and cost estimation.
Defines validation models and protection constants.
"""
from typing import Optional, Dict, Any
from pydantic import BaseModel, Field


# Protection Constants
MAX_TOKENS_PER_JOB = 200
MAX_PRODUCTS_PER_SEARCH = 10
TIMEOUT_PER_JOB = 120
MIN_TOKEN_BALANCE_REQUIRED = 50


class JobValidationResult(BaseModel):
    """Result of job requirement validation."""
    estimated_tokens: int = Field(..., description="Estimated token cost")
    current_balance: int = Field(..., description="Current Keepa balance")
    safe_to_proceed: bool = Field(..., description="Whether job can proceed")
    warning_message: Optional[str] = Field(None, description="Warning if unsafe")


class CostEstimateRequest(BaseModel):
    """Request to estimate job token cost."""
    discovery_config: Dict[str, Any] = Field(..., description="Discovery parameters")


class CostEstimateResponse(BaseModel):
    """Response with cost estimation details."""
    estimated_tokens: int
    current_balance: int
    safe_to_proceed: bool
    warning_message: Optional[str] = None
    max_allowed: int = MAX_TOKENS_PER_JOB
    suggestion: Optional[str] = None
```

### Step 1.4: Run tests to verify they pass

Run:
```bash
cd backend
pytest tests/schemas/test_autosourcing_safeguards_schemas.py -v
```

Expected: 6 PASSED

### Step 1.5: Commit schema implementation

```bash
git add backend/app/schemas/autosourcing_safeguards.py backend/tests/schemas/test_autosourcing_safeguards_schemas.py
git commit -m "feat(safeguards): add AutoSourcing protection schemas and constants

- Define MAX_TOKENS_PER_JOB=200, TIMEOUT_PER_JOB=120, MIN_TOKEN_BALANCE=50
- Add JobValidationResult, CostEstimateRequest, CostEstimateResponse
- All 6 schema tests passing

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 2: Cost Estimation Logic

**Goal:** Implement token cost calculation before job execution

**Files:**
- Create: `backend/app/services/autosourcing_cost_estimator.py`
- Test: `backend/tests/services/test_autosourcing_cost_estimator.py`

### Step 2.1: Write cost estimator tests

Create: `backend/tests/services/test_autosourcing_cost_estimator.py`

```python
"""
Tests for AutoSourcing cost estimation service.
"""
import pytest
from app.services.autosourcing_cost_estimator import AutoSourcingCostEstimator


@pytest.fixture
def estimator():
    """Create estimator instance."""
    return AutoSourcingCostEstimator()


def test_estimate_discovery_cost_small_batch(estimator):
    """Test cost estimation for small product discovery."""
    config = {
        "categories": ["books"],
        "max_results": 10
    }

    cost = estimator.estimate_discovery_cost(config)

    # Product Finder: 10 tokens per page, ~10 results = 1 page = 10 tokens
    # Analysis: 10 products * 1 token = 10 tokens
    # Total: ~20 tokens
    assert cost >= 15
    assert cost <= 25


def test_estimate_discovery_cost_max_batch(estimator):
    """Test cost estimation respects MAX_PRODUCTS_PER_SEARCH."""
    config = {
        "categories": ["books"],
        "max_results": 500  # Exceeds limit
    }

    cost = estimator.estimate_discovery_cost(config)

    # Should cap at MAX_PRODUCTS_PER_SEARCH=10
    # 10 tokens discovery + 10 tokens analysis = 20 tokens
    assert cost >= 15
    assert cost <= 25


def test_estimate_with_multiple_categories(estimator):
    """Test cost scales with multiple categories."""
    config = {
        "categories": ["books", "electronics", "toys"],
        "max_results": 10
    }

    cost = estimator.estimate_discovery_cost(config)

    # 3 categories * ~20 tokens = 60 tokens
    assert cost >= 50
    assert cost <= 70


def test_estimate_total_job_cost(estimator):
    """Test total job cost includes all components."""
    discovery_config = {
        "categories": ["books"],
        "max_results": 10
    }

    total = estimator.estimate_total_job_cost(discovery_config)

    # Discovery + Analysis + Buffer (10%)
    assert total >= 20
    assert total <= 30
```

### Step 2.2: Run tests to verify they fail

Run:
```bash
cd backend
pytest tests/services/test_autosourcing_cost_estimator.py -v
```

Expected: FAIL with "ModuleNotFoundError: app.services.autosourcing_cost_estimator"

### Step 2.3: Implement cost estimator

Create: `backend/app/services/autosourcing_cost_estimator.py`

```python
"""
AutoSourcing Cost Estimator - Calculate token costs before job execution.
Prevents expensive jobs from exhausting token balance.
"""
import logging
from typing import Dict, Any
from app.schemas.autosourcing_safeguards import MAX_PRODUCTS_PER_SEARCH

logger = logging.getLogger(__name__)


class AutoSourcingCostEstimator:
    """
    Estimates Keepa API token costs for AutoSourcing jobs.

    Token Costs:
    - Product Finder: 10 tokens per page (~10 results per page)
    - Product Details: 1 token per ASIN
    """

    PRODUCT_FINDER_COST_PER_PAGE = 10
    RESULTS_PER_PAGE = 10
    PRODUCT_DETAIL_COST = 1
    SAFETY_BUFFER_PCT = 0.10  # 10% buffer for variations

    def estimate_discovery_cost(self, discovery_config: Dict[str, Any]) -> int:
        """
        Estimate token cost for product discovery phase.

        Args:
            discovery_config: Discovery parameters (categories, max_results, etc)

        Returns:
            Estimated tokens for discovery
        """
        max_results = min(
            discovery_config.get("max_results", 50),
            MAX_PRODUCTS_PER_SEARCH
        )

        categories = discovery_config.get("categories", [])
        num_categories = len(categories) if categories else 1

        # Product Finder cost per category
        pages_per_category = max(1, max_results // self.RESULTS_PER_PAGE)
        finder_cost = pages_per_category * self.PRODUCT_FINDER_COST_PER_PAGE * num_categories

        logger.debug(
            f"Discovery cost: {pages_per_category} pages * "
            f"{self.PRODUCT_FINDER_COST_PER_PAGE} tokens * "
            f"{num_categories} categories = {finder_cost} tokens"
        )

        return finder_cost

    def estimate_analysis_cost(self, num_products: int) -> int:
        """
        Estimate token cost for product analysis phase.

        Args:
            num_products: Number of products to analyze

        Returns:
            Estimated tokens for analysis
        """
        capped_products = min(num_products, MAX_PRODUCTS_PER_SEARCH)
        analysis_cost = capped_products * self.PRODUCT_DETAIL_COST

        logger.debug(f"Analysis cost: {capped_products} products * 1 token = {analysis_cost} tokens")

        return analysis_cost

    def estimate_total_job_cost(self, discovery_config: Dict[str, Any]) -> int:
        """
        Estimate total token cost for complete AutoSourcing job.

        Args:
            discovery_config: Discovery configuration

        Returns:
            Total estimated tokens with safety buffer
        """
        max_results = min(
            discovery_config.get("max_results", 50),
            MAX_PRODUCTS_PER_SEARCH
        )

        discovery_cost = self.estimate_discovery_cost(discovery_config)
        analysis_cost = self.estimate_analysis_cost(max_results)

        subtotal = discovery_cost + analysis_cost
        buffer = int(subtotal * self.SAFETY_BUFFER_PCT)
        total = subtotal + buffer

        logger.info(
            f"Total job cost estimate: discovery={discovery_cost}, "
            f"analysis={analysis_cost}, buffer={buffer}, total={total}"
        )

        return total
```

### Step 2.4: Run tests to verify they pass

Run:
```bash
cd backend
pytest tests/services/test_autosourcing_cost_estimator.py -v
```

Expected: 5 PASSED

### Step 2.5: Commit cost estimator

```bash
git add backend/app/services/autosourcing_cost_estimator.py backend/tests/services/test_autosourcing_cost_estimator.py
git commit -m "feat(safeguards): implement AutoSourcing cost estimation service

- Calculate token costs before job execution
- Cap at MAX_PRODUCTS_PER_SEARCH (10 products)
- Include 10% safety buffer for variations
- All 5 cost estimator tests passing

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 3: Job Validation Service

**Goal:** Validate job requirements (tokens, balance, limits) before execution

**Files:**
- Create: `backend/app/services/autosourcing_validator.py`
- Test: `backend/tests/services/test_autosourcing_validator.py`

### Step 3.1: Write validator tests

Create: `backend/tests/services/test_autosourcing_validator.py`

```python
"""
Tests for AutoSourcing job validation service.
"""
import pytest
from unittest.mock import AsyncMock, MagicMock
from fastapi import HTTPException

from app.services.autosourcing_validator import AutoSourcingValidator
from app.schemas.autosourcing_safeguards import (
    MAX_TOKENS_PER_JOB,
    MIN_TOKEN_BALANCE_REQUIRED
)


@pytest.fixture
def mock_keepa_service():
    """Mock KeepaService."""
    service = MagicMock()
    service.get_token_balance = AsyncMock(return_value=1000)
    return service


@pytest.fixture
def validator(mock_keepa_service):
    """Create validator with mocked dependencies."""
    return AutoSourcingValidator(keepa_service=mock_keepa_service)


@pytest.mark.asyncio
async def test_validate_job_accepts_safe_job(validator):
    """Test validation passes for safe job configuration."""
    discovery_config = {
        "categories": ["books"],
        "max_results": 10
    }
    scoring_config = {}

    result = await validator.validate_job_requirements(
        discovery_config, scoring_config
    )

    assert result.safe_to_proceed is True
    assert result.estimated_tokens < MAX_TOKENS_PER_JOB
    assert result.current_balance >= MIN_TOKEN_BALANCE_REQUIRED


@pytest.mark.asyncio
async def test_validate_job_rejects_expensive_job(validator):
    """Test validation rejects job exceeding MAX_TOKENS_PER_JOB."""
    discovery_config = {
        "categories": ["books", "electronics", "toys", "games"],  # 4 categories
        "max_results": 50  # Will be capped but estimate still high
    }
    scoring_config = {}

    # Manually set high cost scenario
    validator.cost_estimator.estimate_total_job_cost = lambda x: 250

    with pytest.raises(HTTPException) as exc:
        await validator.validate_job_requirements(
            discovery_config, scoring_config
        )

    assert exc.value.status_code == 400
    assert "JOB_TOO_EXPENSIVE" in str(exc.value.detail)


@pytest.mark.asyncio
async def test_validate_job_rejects_insufficient_balance(validator, mock_keepa_service):
    """Test validation rejects job when balance too low."""
    # Mock low balance
    mock_keepa_service.get_token_balance = AsyncMock(return_value=10)

    discovery_config = {
        "categories": ["books"],
        "max_results": 10
    }
    scoring_config = {}

    with pytest.raises(HTTPException) as exc:
        await validator.validate_job_requirements(
            discovery_config, scoring_config
        )

    assert exc.value.status_code == 429
    assert "INSUFFICIENT_TOKENS" in str(exc.value.detail)


@pytest.mark.asyncio
async def test_validate_job_provides_helpful_error_messages(validator):
    """Test error messages include actionable suggestions."""
    discovery_config = {
        "categories": ["books", "electronics", "toys", "games", "sports"],
        "max_results": 100
    }
    scoring_config = {}

    validator.cost_estimator.estimate_total_job_cost = lambda x: 300

    with pytest.raises(HTTPException) as exc:
        await validator.validate_job_requirements(
            discovery_config, scoring_config
        )

    error_detail = exc.value.detail
    assert "estimated_tokens" in error_detail
    assert "max_allowed" in error_detail
    assert "suggestion" in error_detail
```

### Step 3.2: Run tests to verify they fail

Run:
```bash
cd backend
pytest tests/services/test_autosourcing_validator.py -v
```

Expected: FAIL with "ModuleNotFoundError: app.services.autosourcing_validator"

### Step 3.3: Implement validator service

Create: `backend/app/services/autosourcing_validator.py`

```python
"""
AutoSourcing Validator - Validate job requirements before execution.
Prevents expensive jobs and insufficient token scenarios.
"""
import logging
from typing import Dict, Any
from fastapi import HTTPException, status

from app.services.keepa_service import KeepaService
from app.services.autosourcing_cost_estimator import AutoSourcingCostEstimator
from app.schemas.autosourcing_safeguards import (
    JobValidationResult,
    MAX_TOKENS_PER_JOB,
    MIN_TOKEN_BALANCE_REQUIRED
)

logger = logging.getLogger(__name__)


class AutoSourcingValidator:
    """
    Validates AutoSourcing job requirements before execution.
    Enforces token limits and balance requirements.
    """

    def __init__(self, keepa_service: KeepaService):
        self.keepa_service = keepa_service
        self.cost_estimator = AutoSourcingCostEstimator()

    async def validate_job_requirements(
        self,
        discovery_config: Dict[str, Any],
        scoring_config: Dict[str, Any]
    ) -> JobValidationResult:
        """
        Validate job can proceed safely within token limits.

        Args:
            discovery_config: Product discovery parameters
            scoring_config: Scoring thresholds

        Returns:
            JobValidationResult with validation status

        Raises:
            HTTPException: 400 if job too expensive, 429 if insufficient tokens
        """
        # Estimate token cost
        estimated_tokens = self.cost_estimator.estimate_total_job_cost(discovery_config)

        # Get current balance
        current_balance = await self.keepa_service.get_token_balance()

        logger.info(
            f"Job validation: estimated={estimated_tokens}, "
            f"balance={current_balance}, max_allowed={MAX_TOKENS_PER_JOB}"
        )

        # Check 1: Job exceeds MAX_TOKENS_PER_JOB
        if estimated_tokens > MAX_TOKENS_PER_JOB:
            logger.warning(
                f"Job rejected: estimated {estimated_tokens} tokens exceeds "
                f"MAX_TOKENS_PER_JOB ({MAX_TOKENS_PER_JOB})"
            )
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail={
                    "error": "JOB_TOO_EXPENSIVE",
                    "estimated_tokens": estimated_tokens,
                    "max_allowed": MAX_TOKENS_PER_JOB,
                    "suggestion": "Reduce max_results or narrow category filters"
                }
            )

        # Check 2: Insufficient token balance
        if current_balance < MIN_TOKEN_BALANCE_REQUIRED:
            logger.warning(
                f"Job rejected: balance {current_balance} below minimum "
                f"required ({MIN_TOKEN_BALANCE_REQUIRED})"
            )
            raise HTTPException(
                status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                detail={
                    "error": "INSUFFICIENT_TOKENS",
                    "balance": current_balance,
                    "required": MIN_TOKEN_BALANCE_REQUIRED,
                    "message": "Token balance too low. Wait for auto-refill (50 tokens per 3 hours)."
                }
            )

        # Validation passed
        return JobValidationResult(
            estimated_tokens=estimated_tokens,
            current_balance=current_balance,
            safe_to_proceed=True,
            warning_message=None
        )
```

### Step 3.4: Run tests to verify they pass

Run:
```bash
cd backend
pytest tests/services/test_autosourcing_validator.py -v
```

Expected: 4 PASSED

### Step 3.5: Commit validator implementation

```bash
git add backend/app/services/autosourcing_validator.py backend/tests/services/test_autosourcing_validator.py
git commit -m "feat(safeguards): implement job validation service

- Validate jobs before execution (cost + balance checks)
- Reject jobs exceeding MAX_TOKENS_PER_JOB (200)
- Reject if balance below MIN_TOKEN_BALANCE (50)
- Provide actionable error messages
- All 4 validator tests passing

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 4: Cost Estimation API Endpoint

**Goal:** Expose /estimate endpoint for frontend cost preview

**Files:**
- Modify: `backend/app/api/v1/routers/autosourcing.py` (add endpoint)
- Test: Update `backend/tests/test_autosourcing_safeguards.py` (API tests)

### Step 4.1: Write API endpoint tests

Create: `backend/tests/api/test_autosourcing_estimate_endpoint.py`

```python
"""
Tests for AutoSourcing cost estimation API endpoint.
"""
import pytest
from httpx import AsyncClient
from app.main import app


@pytest.mark.asyncio
async def test_estimate_endpoint_returns_cost(async_client: AsyncClient):
    """Test /estimate endpoint returns token cost estimate."""
    request_data = {
        "discovery_config": {
            "categories": ["books"],
            "max_results": 10
        }
    }

    response = await async_client.post(
        "/api/v1/autosourcing/estimate",
        json=request_data
    )

    assert response.status_code == 200
    data = response.json()

    assert "estimated_tokens" in data
    assert "current_balance" in data
    assert "safe_to_proceed" in data
    assert "max_allowed" in data

    assert data["estimated_tokens"] > 0
    assert data["max_allowed"] == 200


@pytest.mark.asyncio
async def test_estimate_endpoint_warns_expensive_job(async_client: AsyncClient):
    """Test /estimate warns when job cost exceeds limit."""
    request_data = {
        "discovery_config": {
            "categories": ["books", "electronics", "toys", "games"],
            "max_results": 100
        }
    }

    response = await async_client.post(
        "/api/v1/autosourcing/estimate",
        json=request_data
    )

    assert response.status_code == 200
    data = response.json()

    # Should have warning but not reject (estimate only)
    assert data["safe_to_proceed"] is False
    assert data["warning_message"] is not None
    assert "exceed" in data["warning_message"].lower()


@pytest.mark.asyncio
async def test_estimate_endpoint_validates_request_schema(async_client: AsyncClient):
    """Test /estimate validates request schema."""
    invalid_request = {
        "wrong_field": "invalid"
    }

    response = await async_client.post(
        "/api/v1/autosourcing/estimate",
        json=invalid_request
    )

    assert response.status_code == 422  # Validation error
```

### Step 4.2: Run tests to verify they fail

Run:
```bash
cd backend
pytest tests/api/test_autosourcing_estimate_endpoint.py -v
```

Expected: FAIL with "404 Not Found" (endpoint doesn't exist)

### Step 4.3: Add /estimate endpoint to router

Modify: `backend/app/api/v1/routers/autosourcing.py`

Add after imports (around line 18):
```python
from app.services.autosourcing_validator import AutoSourcingValidator
from app.services.autosourcing_cost_estimator import AutoSourcingCostEstimator
from app.schemas.autosourcing_safeguards import (
    CostEstimateRequest,
    CostEstimateResponse,
    MAX_TOKENS_PER_JOB
)
```

Add after dependency injection section (around line 140):
```python
async def get_validator(
    db: AsyncSession = Depends(get_db_session)
) -> AutoSourcingValidator:
    """Dependency to get AutoSourcing validator."""
    settings = get_settings()
    keepa_service = KeepaService(api_key=settings.keepa_api_key)
    return AutoSourcingValidator(keepa_service)
```

Add endpoint after existing routes (around line 250):
```python
@router.post("/estimate", response_model=CostEstimateResponse)
async def estimate_job_cost(
    request: CostEstimateRequest,
    validator: AutoSourcingValidator = Depends(get_validator)
):
    """
    Estimate token cost for AutoSourcing job BEFORE execution.

    Allows frontend to display cost estimate and warn user.
    Does NOT execute the job, only estimates cost.

    **Token Cost:** 0 tokens (estimation only, no Keepa API calls)

    **Returns:**
    - estimated_tokens: Calculated token cost
    - current_balance: User's current token balance
    - safe_to_proceed: Whether job is within limits
    - warning_message: Warning if job unsafe
    - max_allowed: MAX_TOKENS_PER_JOB limit (200)
    - suggestion: How to reduce cost if needed
    """
    estimator = AutoSourcingCostEstimator()
    estimated_tokens = estimator.estimate_total_job_cost(request.discovery_config)

    # Get current balance
    current_balance = await validator.keepa_service.get_token_balance()

    # Check if job safe to proceed
    safe_to_proceed = estimated_tokens <= MAX_TOKENS_PER_JOB
    warning_message = None
    suggestion = None

    if not safe_to_proceed:
        warning_message = (
            f"Estimated cost ({estimated_tokens} tokens) exceeds "
            f"maximum allowed per job ({MAX_TOKENS_PER_JOB} tokens)"
        )
        suggestion = "Reduce max_results or narrow category filters"

    return CostEstimateResponse(
        estimated_tokens=estimated_tokens,
        current_balance=current_balance,
        safe_to_proceed=safe_to_proceed,
        warning_message=warning_message,
        max_allowed=MAX_TOKENS_PER_JOB,
        suggestion=suggestion
    )
```

### Step 4.4: Run tests to verify they pass

Run:
```bash
cd backend
pytest tests/api/test_autosourcing_estimate_endpoint.py -v
```

Expected: 3 PASSED

### Step 4.5: Commit estimate endpoint

```bash
git add backend/app/api/v1/routers/autosourcing.py backend/tests/api/test_autosourcing_estimate_endpoint.py
git commit -m "feat(safeguards): add /estimate endpoint for cost preview

- POST /api/v1/autosourcing/estimate returns token cost
- Frontend can display estimate BEFORE job submission
- Warns if job exceeds MAX_TOKENS_PER_JOB
- Provides suggestions to reduce cost
- All 3 API endpoint tests passing

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 5: Integrate Validation into run_custom Endpoint

**Goal:** Enforce validation in existing job submission endpoint

**Files:**
- Modify: `backend/app/api/v1/routers/autosourcing.py:144-200` (run_custom_search endpoint)
- Test: Update existing tests

### Step 5.1: Write integration tests

Create: `backend/tests/api/test_autosourcing_run_custom_validation.py`

```python
"""
Tests for AutoSourcing run_custom endpoint with validation.
"""
import pytest
from httpx import AsyncClient
from unittest.mock import patch


@pytest.mark.asyncio
async def test_run_custom_rejects_expensive_job(async_client: AsyncClient):
    """Test run_custom rejects job exceeding token limit."""
    request_data = {
        "profile_name": "Expensive Test",
        "discovery_config": {
            "categories": ["books", "electronics", "toys", "games"],
            "max_results": 100
        },
        "scoring_config": {
            "roi_min": 30,
            "velocity_min": 70
        }
    }

    # Mock cost estimator to return high cost
    with patch(
        'app.services.autosourcing_cost_estimator.AutoSourcingCostEstimator.estimate_total_job_cost',
        return_value=300
    ):
        response = await async_client.post(
            "/api/v1/autosourcing/run_custom",
            json=request_data
        )

    assert response.status_code == 400
    data = response.json()

    assert "detail" in data
    assert "JOB_TOO_EXPENSIVE" in str(data["detail"])


@pytest.mark.asyncio
async def test_run_custom_accepts_safe_job(async_client: AsyncClient):
    """Test run_custom accepts job within limits."""
    request_data = {
        "profile_name": "Safe Test",
        "discovery_config": {
            "categories": ["books"],
            "max_results": 5
        },
        "scoring_config": {
            "roi_min": 30,
            "velocity_min": 70
        }
    }

    response = await async_client.post(
        "/api/v1/autosourcing/run_custom",
        json=request_data
    )

    # Should accept (or fail later for other reasons, but not validation)
    assert response.status_code in [200, 201, 429]  # 429 if real token check fails
```

### Step 5.2: Run tests to verify they fail

Run:
```bash
cd backend
pytest tests/api/test_autosourcing_run_custom_validation.py -v
```

Expected: FAIL (endpoint doesn't call validator yet)

### Step 5.3: Add validation to run_custom endpoint

Modify: `backend/app/api/v1/routers/autosourcing.py`

Update run_custom_search function signature (line ~145):
```python
@router.post("/run_custom", response_model=AutoSourcingJobResponse)
async def run_custom_search(
    request: RunCustomSearchRequest,
    service: AutoSourcingService = Depends(get_autosourcing_service),
    validator: AutoSourcingValidator = Depends(get_validator)
):
```

Add validation BEFORE service call (line ~165):
```python
    # PHASE 7.0: Validate job requirements before execution
    await validator.validate_job_requirements(
        discovery_config=request.discovery_config.dict(),
        scoring_config=request.scoring_config.dict()
    )

    # Proceed with job execution if validation passed
    job = await service.run_custom_search(
        discovery_config=request.discovery_config.dict(),
        scoring_config=request.scoring_config.dict(),
        profile_name=request.profile_name,
        profile_id=request.profile_id
    )
```

### Step 5.4: Run tests to verify they pass

Run:
```bash
cd backend
pytest tests/api/test_autosourcing_run_custom_validation.py -v
```

Expected: 2 PASSED

### Step 5.5: Commit validation integration

```bash
git add backend/app/api/v1/routers/autosourcing.py backend/tests/api/test_autosourcing_run_custom_validation.py
git commit -m "feat(safeguards): integrate validation into run_custom endpoint

- Validate job requirements BEFORE execution
- Reject expensive jobs (>200 tokens)
- Reject if balance insufficient (<50 tokens)
- All 2 integration tests passing

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 6: Timeout Protection

**Goal:** Enforce 120-second timeout on AutoSourcing jobs

**Files:**
- Modify: `backend/app/services/autosourcing_service.py:44-100` (run_custom_search method)
- Test: `backend/tests/services/test_autosourcing_timeout.py`

### Step 6.1: Write timeout tests

Create: `backend/tests/services/test_autosourcing_timeout.py`

```python
"""
Tests for AutoSourcing timeout protection.
"""
import pytest
import asyncio
from unittest.mock import AsyncMock, MagicMock, patch
from fastapi import HTTPException

from app.services.autosourcing_service import AutoSourcingService
from app.schemas.autosourcing_safeguards import TIMEOUT_PER_JOB


@pytest.fixture
def mock_db_session():
    """Mock database session."""
    session = MagicMock()
    session.add = MagicMock()
    session.commit = AsyncMock()
    session.refresh = AsyncMock()
    return session


@pytest.fixture
def mock_keepa_service():
    """Mock Keepa service."""
    service = MagicMock()
    service.can_perform_action = AsyncMock(
        return_value={"can_proceed": True, "current_balance": 1000, "required_tokens": 50}
    )
    return service


@pytest.fixture
def autosourcing_service(mock_db_session, mock_keepa_service):
    """Create AutoSourcingService with mocks."""
    return AutoSourcingService(mock_db_session, mock_keepa_service)


@pytest.mark.asyncio
async def test_timeout_protection_cancels_long_job(autosourcing_service):
    """Test timeout protection cancels jobs exceeding TIMEOUT_PER_JOB."""
    discovery_config = {"categories": ["books"], "max_results": 10}
    scoring_config = {"roi_min": 30}

    # Mock slow operation
    async def slow_discovery(*args, **kwargs):
        await asyncio.sleep(TIMEOUT_PER_JOB + 10)  # Exceed timeout
        return []

    with patch.object(
        autosourcing_service,
        '_discover_products',
        side_effect=slow_discovery
    ):
        with pytest.raises(asyncio.TimeoutError):
            await autosourcing_service.run_custom_search(
                discovery_config=discovery_config,
                scoring_config=scoring_config,
                profile_name="Timeout Test"
            )


@pytest.mark.asyncio
async def test_timeout_allows_fast_jobs(autosourcing_service):
    """Test timeout allows jobs completing within limit."""
    discovery_config = {"categories": ["books"], "max_results": 5}
    scoring_config = {"roi_min": 30}

    # Mock fast operation
    async def fast_discovery(*args, **kwargs):
        await asyncio.sleep(0.1)  # Fast
        return []

    with patch.object(
        autosourcing_service,
        '_discover_products',
        side_effect=fast_discovery
    ):
        # Should not raise TimeoutError
        job = await autosourcing_service.run_custom_search(
            discovery_config=discovery_config,
            scoring_config=scoring_config,
            profile_name="Fast Test"
        )

        assert job is not None
```

### Step 6.2: Run tests to verify they fail

Run:
```bash
cd backend
pytest tests/services/test_autosourcing_timeout.py -v
```

Expected: FAIL (timeout not implemented)

### Step 6.3: Add timeout to run_custom_search

Modify: `backend/app/services/autosourcing_service.py`

Add import at top:
```python
from app.schemas.autosourcing_safeguards import TIMEOUT_PER_JOB
```

Wrap job execution in timeout (around line 100):
```python
    async def run_custom_search(
        self,
        discovery_config: Dict[str, Any],
        scoring_config: Dict[str, Any],
        profile_name: str,
        profile_id: Optional[UUID] = None
    ) -> AutoSourcingJob:
        # ... existing token check code ...

        logger.info(f"Starting AutoSourcing job: {profile_name}")

        # Create job record
        job = AutoSourcingJob(
            profile_name=profile_name,
            profile_id=profile_id,
            discovery_config=discovery_config,
            scoring_config=scoring_config,
            status=JobStatus.RUNNING,
            launched_at=datetime.utcnow()
        )

        self.db.add(job)
        await self.db.commit()
        await self.db.refresh(job)

        try:
            # PHASE 7.0: Enforce timeout protection
            async with asyncio.timeout(TIMEOUT_PER_JOB):
                # Execute job logic with timeout
                job = await self._execute_job_logic(job, discovery_config, scoring_config)

        except asyncio.TimeoutError:
            logger.error(
                f"Job {job.id} exceeded timeout ({TIMEOUT_PER_JOB}s). "
                f"Marking as TIMEOUT."
            )
            job.status = JobStatus.FAILED
            job.completed_at = datetime.utcnow()
            job.duration_ms = int((job.completed_at - job.launched_at).total_seconds() * 1000)
            await self.db.commit()

            raise HTTPException(
                status_code=408,
                detail={
                    "error": "JOB_TIMEOUT",
                    "timeout_seconds": TIMEOUT_PER_JOB,
                    "message": "Job exceeded timeout limit. Reduce search scope."
                }
            )

        return job

    async def _execute_job_logic(
        self,
        job: AutoSourcingJob,
        discovery_config: Dict[str, Any],
        scoring_config: Dict[str, Any]
    ) -> AutoSourcingJob:
        """Execute job logic (extracted for timeout wrapping)."""
        # ... move existing job execution code here ...
```

### Step 6.4: Run tests to verify they pass

Run:
```bash
cd backend
pytest tests/services/test_autosourcing_timeout.py -v
```

Expected: 2 PASSED

### Step 6.5: Commit timeout protection

```bash
git add backend/app/services/autosourcing_service.py backend/tests/services/test_autosourcing_timeout.py
git commit -m "feat(safeguards): add timeout protection to AutoSourcing jobs

- Enforce TIMEOUT_PER_JOB (120 seconds)
- Cancel long-running jobs automatically
- Return HTTP 408 with helpful error message
- All 2 timeout tests passing

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 7: Deduplication Logic

**Goal:** Prevent analyzing same ASIN multiple times in one job

**Files:**
- Modify: `backend/app/services/autosourcing_service.py` (add deduplication)
- Test: `backend/tests/services/test_autosourcing_deduplication.py`

### Step 7.1: Write deduplication tests

Create: `backend/tests/services/test_autosourcing_deduplication.py`

```python
"""
Tests for AutoSourcing ASIN deduplication.
"""
import pytest
from unittest.mock import AsyncMock, MagicMock
from app.services.autosourcing_service import AutoSourcingService


@pytest.fixture
def service_with_duplicates(mock_db_session, mock_keepa_service):
    """Create service that will discover duplicate ASINs."""
    service = AutoSourcingService(mock_db_session, mock_keepa_service)

    # Mock discovery to return duplicates
    async def mock_discover(*args, **kwargs):
        return [
            {"asin": "ASIN1", "title": "Product 1"},
            {"asin": "ASIN2", "title": "Product 2"},
            {"asin": "ASIN1", "title": "Product 1 Duplicate"},  # Duplicate
            {"asin": "ASIN3", "title": "Product 3"},
            {"asin": "ASIN2", "title": "Product 2 Duplicate"},  # Duplicate
        ]

    service._discover_products = mock_discover
    return service


@pytest.mark.asyncio
async def test_deduplication_removes_duplicate_asins(service_with_duplicates):
    """Test deduplication removes duplicate ASINs from analysis."""
    discovery_config = {"categories": ["books"], "max_results": 10}
    scoring_config = {"roi_min": 30}

    # Mock analysis to track calls
    analyzed_asins = []

    async def track_analysis(asin, *args, **kwargs):
        analyzed_asins.append(asin)
        return {"asin": asin, "roi": 50}

    service_with_duplicates._analyze_single_product = track_analysis

    job = await service_with_duplicates.run_custom_search(
        discovery_config=discovery_config,
        scoring_config=scoring_config,
        profile_name="Dedup Test"
    )

    # Should only analyze unique ASINs
    assert len(analyzed_asins) == 3  # ASIN1, ASIN2, ASIN3
    assert "ASIN1" in analyzed_asins
    assert "ASIN2" in analyzed_asins
    assert "ASIN3" in analyzed_asins

    # Should not analyze duplicates
    assert analyzed_asins.count("ASIN1") == 1
    assert analyzed_asins.count("ASIN2") == 1


@pytest.mark.asyncio
async def test_deduplication_preserves_order(service_with_duplicates):
    """Test deduplication preserves first occurrence order."""
    discovery_config = {"categories": ["books"], "max_results": 10}
    scoring_config = {"roi_min": 30}

    analyzed_order = []

    async def track_order(asin, *args, **kwargs):
        analyzed_order.append(asin)
        return {"asin": asin, "roi": 50}

    service_with_duplicates._analyze_single_product = track_order

    await service_with_duplicates.run_custom_search(
        discovery_config=discovery_config,
        scoring_config=scoring_config,
        profile_name="Order Test"
    )

    # Should preserve first-seen order
    assert analyzed_order == ["ASIN1", "ASIN2", "ASIN3"]
```

### Step 7.2: Run tests to verify they fail

Run:
```bash
cd backend
pytest tests/services/test_autosourcing_deduplication.py -v
```

Expected: FAIL (deduplication not implemented)

### Step 7.3: Implement deduplication in _execute_job_logic

Modify: `backend/app/services/autosourcing_service.py`

Update `_execute_job_logic` method to track analyzed ASINs:

```python
    async def _execute_job_logic(
        self,
        job: AutoSourcingJob,
        discovery_config: Dict[str, Any],
        scoring_config: Dict[str, Any]
    ) -> AutoSourcingJob:
        """Execute job logic with deduplication."""
        start_time = datetime.utcnow()

        try:
            # Step 1: Discover products via Keepa
            discovered_products = await self._discover_products(discovery_config)
            logger.info(f"Discovered {len(discovered_products)} products")

            # PHASE 7.0: Deduplication - track analyzed ASINs
            analyzed_asins = set()
            picks = []

            # Step 2: Analyze and score each unique product
            for product in discovered_products:
                asin = product.get("asin")

                # Skip if already analyzed
                if asin in analyzed_asins:
                    logger.debug(f"Skipping duplicate ASIN: {asin}")
                    continue

                # Analyze product
                result = await self._analyze_single_product(
                    asin=asin,
                    product_data=product,
                    scoring_config=scoring_config
                )

                # Mark as analyzed
                analyzed_asins.add(asin)

                # Store if meets criteria
                if result.get("passes_filters"):
                    picks.append(result)

            logger.info(
                f"Analyzed {len(analyzed_asins)} unique products, "
                f"found {len(picks)} qualifying picks"
            )

            # Step 3: Save picks to database
            await self._save_picks_to_job(job, picks)

            # Update job status
            job.status = JobStatus.COMPLETED
            job.completed_at = datetime.utcnow()
            job.duration_ms = int((job.completed_at - start_time).total_seconds() * 1000)
            job.total_tested = len(analyzed_asins)
            job.total_selected = len(picks)

            await self.db.commit()
            await self.db.refresh(job, ["picks"])

            return job

        except Exception as e:
            logger.error(f"Job execution failed: {e}")
            job.status = JobStatus.FAILED
            job.completed_at = datetime.utcnow()
            await self.db.commit()
            raise
```

### Step 7.4: Run tests to verify they pass

Run:
```bash
cd backend
pytest tests/services/test_autosourcing_deduplication.py -v
```

Expected: 2 PASSED

### Step 7.5: Commit deduplication implementation

```bash
git add backend/app/services/autosourcing_service.py backend/tests/services/test_autosourcing_deduplication.py
git commit -m "feat(safeguards): implement ASIN deduplication in AutoSourcing

- Track analyzed ASINs to prevent duplicate analysis
- Skip duplicate ASINs from Product Finder results
- Preserve first-occurrence order
- All 2 deduplication tests passing

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 8: E2E Tests for Safeguards

**Goal:** Add Playwright E2E tests validating safeguards in production

**Files:**
- Create: `backend/tests/e2e/tests/08-autosourcing-safeguards.spec.js`
- Modify: `.github/workflows/e2e-monitoring.yml` (add job)

### Step 8.1: Write E2E test suite

Create: `backend/tests/e2e/tests/08-autosourcing-safeguards.spec.js`

```javascript
/**
 * E2E Tests - AutoSourcing Safeguards
 *
 * Validates production safeguards:
 * - Cost estimation before job submission
 * - Job rejection if exceeds MAX_TOKENS_PER_JOB
 * - Timeout protection (120 seconds)
 *
 * Token Cost: ~50 tokens (3 real jobs with small batches)
 */
const { test, expect } = require('@playwright/test');

const BACKEND_URL = process.env.BACKEND_URL || 'https://arbitragevault-backend-v2.onrender.com';
const FRONTEND_URL = process.env.FRONTEND_URL || 'https://arbitragevault.netlify.app';

test.describe('AutoSourcing Safeguards', () => {

  test('Should return cost estimate from /estimate endpoint', async ({ request }) => {
    const response = await request.post(`${BACKEND_URL}/api/v1/autosourcing/estimate`, {
      data: {
        discovery_config: {
          categories: ['books'],
          max_results: 10
        }
      }
    });

    expect(response.ok()).toBeTruthy();
    const data = await response.json();

    // Verify response structure
    expect(data).toHaveProperty('estimated_tokens');
    expect(data).toHaveProperty('current_balance');
    expect(data).toHaveProperty('safe_to_proceed');
    expect(data).toHaveProperty('max_allowed');

    // Verify reasonable values
    expect(data.estimated_tokens).toBeGreaterThan(0);
    expect(data.estimated_tokens).toBeLessThan(200);
    expect(data.max_allowed).toBe(200);
  });

  test('Should warn when job cost exceeds MAX_TOKENS_PER_JOB', async ({ request }) => {
    const response = await request.post(`${BACKEND_URL}/api/v1/autosourcing/estimate`, {
      data: {
        discovery_config: {
          categories: ['books', 'electronics', 'toys', 'games'],
          max_results: 100
        }
      }
    });

    expect(response.ok()).toBeTruthy();
    const data = await response.json();

    // Should warn about expensive job
    expect(data.safe_to_proceed).toBe(false);
    expect(data.warning_message).not.toBeNull();
    expect(data.warning_message.toLowerCase()).toContain('exceed');
    expect(data.suggestion).not.toBeNull();
  });

  test('Should reject job submission if cost exceeds limit', async ({ request }) => {
    // Mock expensive job by manipulating config
    const response = await request.post(`${BACKEND_URL}/api/v1/autosourcing/run_custom`, {
      data: {
        profile_name: 'Expensive Test Job',
        discovery_config: {
          categories: ['books', 'electronics', 'toys', 'games', 'sports'],
          max_results: 100
        },
        scoring_config: {
          roi_min: 30,
          velocity_min: 70
        }
      }
    });

    // Should reject with HTTP 400
    expect(response.status()).toBe(400);
    const error = await response.json();

    expect(error.detail).toHaveProperty('error');
    expect(error.detail.error).toBe('JOB_TOO_EXPENSIVE');
    expect(error.detail).toHaveProperty('estimated_tokens');
    expect(error.detail).toHaveProperty('suggestion');
  });

});
```

### Step 8.2: Run E2E tests locally

Run:
```bash
cd backend/tests/e2e
npx playwright test tests/08-autosourcing-safeguards.spec.js
```

Expected: 3 PASSED (uses real production API)

### Step 8.3: Add GitHub Actions job

Modify: `.github/workflows/e2e-monitoring.yml`

Add job after existing jobs:

```yaml
  autosourcing-safeguards:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: backend/tests/e2e/package-lock.json

      - name: Install dependencies
        working-directory: backend/tests/e2e
        run: npm ci

      - name: Install Playwright browsers
        working-directory: backend/tests/e2e
        run: npx playwright install chromium

      - name: Run AutoSourcing Safeguards tests
        working-directory: backend/tests/e2e
        env:
          BACKEND_URL: https://arbitragevault-backend-v2.onrender.com
          FRONTEND_URL: https://arbitragevault.netlify.app
        run: npx playwright test tests/08-autosourcing-safeguards.spec.js

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: safeguards-test-results
          path: backend/tests/e2e/test-results/
          retention-days: 7
```

### Step 8.4: Commit E2E tests

```bash
git add backend/tests/e2e/tests/08-autosourcing-safeguards.spec.js .github/workflows/e2e-monitoring.yml
git commit -m "test(e2e): add AutoSourcing safeguards E2E tests

- Test /estimate endpoint returns cost
- Test warning when job exceeds MAX_TOKENS_PER_JOB
- Test job rejection if too expensive
- Add GitHub Actions job for safeguards tests
- All 3 E2E tests passing

Token Cost: ~50 tokens per run

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 9: Documentation and Memory Update

**Goal:** Document Phase 7.0 completion and update project memory

**Files:**
- Create: `docs/AUTOSOURCING_SAFEGUARDS.md`
- Update: `.claude/memory/compact_master.md`
- Update: `.claude/memory/compact_current.md`

### Step 9.1: Create safeguards documentation

Create: `docs/AUTOSOURCING_SAFEGUARDS.md`

```markdown
# AutoSourcing Production Safeguards

**Date:** 13 Novembre 2025
**Phase:** 7.0
**Status:** COMPLETE

---

## Overview

Phase 7.0 implements comprehensive production safeguards to prevent token exhaustion and ensure sustainable AutoSourcing operations.

## Protection Limits

| Limit | Value | Purpose |
|-------|-------|---------|
| MAX_TOKENS_PER_JOB | 200 | Maximum tokens per single job |
| MAX_PRODUCTS_PER_SEARCH | 10 | Maximum products analyzed per job |
| TIMEOUT_PER_JOB | 120 seconds | Maximum job execution time |
| MIN_TOKEN_BALANCE_REQUIRED | 50 | Minimum balance to start job |

## Features

### 1. Cost Estimation (`/estimate` endpoint)

**Endpoint:** `POST /api/v1/autosourcing/estimate`

**Request:**
```json
{
  "discovery_config": {
    "categories": ["books"],
    "max_results": 10
  }
}
```

**Response:**
```json
{
  "estimated_tokens": 25,
  "current_balance": 1200,
  "safe_to_proceed": true,
  "warning_message": null,
  "max_allowed": 200,
  "suggestion": null
}
```

**Token Cost:** 0 tokens (estimation only)

### 2. Job Validation

**When:** Before job execution in `/run_custom` endpoint

**Checks:**
1. Estimated cost <= MAX_TOKENS_PER_JOB (200)
2. Current balance >= MIN_TOKEN_BALANCE_REQUIRED (50)

**Error Responses:**

HTTP 400 - Job too expensive:
```json
{
  "error": "JOB_TOO_EXPENSIVE",
  "estimated_tokens": 250,
  "max_allowed": 200,
  "suggestion": "Reduce max_results or narrow category filters"
}
```

HTTP 429 - Insufficient tokens:
```json
{
  "error": "INSUFFICIENT_TOKENS",
  "balance": 30,
  "required": 50,
  "message": "Token balance too low. Wait for auto-refill (50 tokens per 3 hours)."
}
```

### 3. Timeout Protection

**Limit:** 120 seconds per job

**Behavior:** Jobs exceeding timeout are automatically cancelled

**Error Response:**
```json
{
  "error": "JOB_TIMEOUT",
  "timeout_seconds": 120,
  "message": "Job exceeded timeout limit. Reduce search scope."
}
```

### 4. ASIN Deduplication

**Feature:** Automatically skip duplicate ASINs in discovery results

**Benefit:** Prevents wasting tokens analyzing same product multiple times

**Implementation:** Tracks analyzed ASINs in set(), preserves first-occurrence order

## Testing

### Backend Unit Tests
- **File:** `backend/tests/test_autosourcing_safeguards.py`
- **Tests:** 15 tests covering all safeguard features
- **Status:** 15/15 PASSING

### E2E Tests
- **File:** `backend/tests/e2e/tests/08-autosourcing-safeguards.spec.js`
- **Tests:** 3 tests validating production endpoints
- **Token Cost:** ~50 tokens per run
- **Status:** 3/3 PASSING

## Usage Examples

### Frontend Cost Preview

```typescript
const estimateJobCost = async (config: JobConfig): Promise<CostEstimate> => {
  const response = await fetch('/api/v1/autosourcing/estimate', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ discovery_config: config })
  });

  return await response.json();
};

// Display cost before submission
const estimate = await estimateJobCost(userConfig);
if (!estimate.safe_to_proceed) {
  alert(estimate.warning_message);
}
```

### Backend Validation

Validation happens automatically in `/run_custom` endpoint:

```python
@router.post("/run_custom", response_model=AutoSourcingJobResponse)
async def run_custom_search(
    request: RunCustomSearchRequest,
    validator: AutoSourcingValidator = Depends(get_validator)
):
    # Automatic validation
    await validator.validate_job_requirements(
        discovery_config=request.discovery_config.dict(),
        scoring_config=request.scoring_config.dict()
    )

    # Proceed only if validation passed
    ...
```

## Impact

**Before Phase 7.0:**
- Jobs could consume 500+ tokens
- No cost preview for users
- No timeout protection
- Duplicate ASINs analyzed multiple times
- Risk of complete token exhaustion

**After Phase 7.0:**
- Jobs capped at 200 tokens maximum
- Frontend displays cost estimate
- 120-second timeout enforced
- Deduplication prevents waste
- Production sustainability guaranteed

---

**Authors:**
- Aziz Traore
- Claude (Anthropic AI Assistant)

**Date:** 13 Novembre 2025
**Version:** Phase 7.0 Complete
```

### Step 9.2: Update compact_master.md

Modify: `.claude/memory/compact_master.md`

Add Phase 7.0 section after Phase 6:

```markdown
### Phase 7.0 - AutoSourcing Safeguards COMPLETE
**Periode** : 13 Novembre 2025
**Status** : COMPLETE

**Objectif** : Protection production contre epuisement tokens

**Livrables** :
1. Backend Protection Limits (MAX_TOKENS=200, TIMEOUT=120s)
2. Cost Estimation Endpoint (`/estimate`)
3. Job Validation Service
4. Timeout Protection (asyncio.timeout)
5. ASIN Deduplication Logic

**Tests** :
- Backend unit tests : 15/15 PASSING
- E2E tests : 3/3 PASSING (~50 tokens per run)

**Files Created** :
- `backend/app/schemas/autosourcing_safeguards.py`
- `backend/app/services/autosourcing_cost_estimator.py`
- `backend/app/services/autosourcing_validator.py`
- `backend/tests/e2e/tests/08-autosourcing-safeguards.spec.js`
- `docs/AUTOSOURCING_SAFEGUARDS.md`

**Commits Cles** :
- Schema implementation (6 tests)
- Cost estimator (5 tests)
- Validator service (4 tests)
- /estimate endpoint (3 API tests)
- Validation integration (2 tests)
- Timeout protection (2 tests)
- Deduplication (2 tests)
- E2E tests (3 tests)

**Impact Production** :
- Jobs cannot exceed 200 tokens
- Users see cost estimate before submission
- Timeout protection prevents runaway jobs
- Deduplication eliminates waste
```

### Step 9.3: Update compact_current.md

Modify: `.claude/memory/compact_current.md`

Update header and add Phase 7.0 section:

```markdown
**Derniere mise a jour** : 2025-11-13
**Session** : Phase 7.0 AutoSourcing Safeguards Implementation
**Phase** : Phase 7.0 COMPLETE

---

## [2025-11-13] PHASE 7.0 - AutoSourcing Safeguards COMPLETE

**Contexte** : Critical production protection against token exhaustion

**Accomplissements** :
1. Backend safeguards implemented (15 unit tests PASSING)
2. Cost estimation endpoint added
3. Job validation integrated into run_custom
4. Timeout protection enforced (120s)
5. ASIN deduplication prevents waste
6. E2E tests validate production behavior (3/3 PASSING)
7. Comprehensive documentation created

**Token Costs** :
- Development/testing : ~50 tokens
- Per E2E run : ~50 tokens
- Production savings : Unlimited (prevents 500+ token jobs)

**Next Priority** : Phase 7.1 - TokenErrorAlert Component (frontend UX)
```

### Step 9.4: Commit documentation updates

```bash
git add docs/AUTOSOURCING_SAFEGUARDS.md .claude/memory/compact_master.md .claude/memory/compact_current.md
git commit -m "docs(phase7.0): complete AutoSourcing safeguards documentation

- Create comprehensive safeguards guide
- Update compact_master.md with Phase 7.0 completion
- Update compact_current.md session summary
- Document all limits, features, tests, impact

Phase 7.0 COMPLETE - Production protection implemented

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Success Criteria Validation

**Phase 7.0 Checklist:**

- [x] AutoSourcing jobs capped at 200 tokens max
- [x] Deduplication eliminates duplicate analyses
- [x] Timeout protection enforced (120 seconds)
- [x] Frontend can request cost estimate via `/estimate`
- [x] Backend unit tests 100% passing (15/15)
- [x] E2E tests 100% passing (3/3)
- [x] Documentation complete

**Test Summary:**

| Category | Tests | Status | Token Cost |
|----------|-------|--------|------------|
| Schemas | 6 | PASS | 0 |
| Cost Estimator | 5 | PASS | 0 |
| Validator | 4 | PASS | 0 |
| API Endpoints | 5 | PASS | 0 |
| Timeout | 2 | PASS | 0 |
| Deduplication | 2 | PASS | 0 |
| E2E Production | 3 | PASS | ~50 |
| **TOTAL** | **27** | **100%** | **~50** |

---

## Deliverables Summary

### Backend Files Created (5)
1. `backend/app/schemas/autosourcing_safeguards.py` - Protection schemas
2. `backend/app/services/autosourcing_cost_estimator.py` - Cost calculation
3. `backend/app/services/autosourcing_validator.py` - Job validation
4. `backend/tests/e2e/tests/08-autosourcing-safeguards.spec.js` - E2E tests
5. `docs/AUTOSOURCING_SAFEGUARDS.md` - Documentation

### Backend Files Modified (3)
1. `backend/app/api/v1/routers/autosourcing.py` - Added /estimate, validation
2. `backend/app/services/autosourcing_service.py` - Timeout, deduplication
3. `.github/workflows/e2e-monitoring.yml` - Added safeguards job

### Test Files Created (7)
1. `backend/tests/schemas/test_autosourcing_safeguards_schemas.py`
2. `backend/tests/services/test_autosourcing_cost_estimator.py`
3. `backend/tests/services/test_autosourcing_validator.py`
4. `backend/tests/api/test_autosourcing_estimate_endpoint.py`
5. `backend/tests/api/test_autosourcing_run_custom_validation.py`
6. `backend/tests/services/test_autosourcing_timeout.py`
7. `backend/tests/services/test_autosourcing_deduplication.py`

### Documentation Files (3)
1. `docs/AUTOSOURCING_SAFEGUARDS.md` - Feature documentation
2. `.claude/memory/compact_master.md` - Updated
3. `.claude/memory/compact_current.md` - Updated

---

## Plan Complete

**Total Tasks:** 9 major tasks, 45 steps
**Estimated Time:** 1-2 days
**Token Cost:** ~50 tokens for testing
**Production Impact:** Prevents unlimited token consumption

Plan saved to: `docs/plans/2025-11-13-phase-7.0-autosourcing-safeguards.md`

---

**Execution Options:**

1. **Subagent-Driven (this session)** - Dispatch fresh subagent per task, review between tasks, fast iteration with quality gates

2. **Parallel Session (separate)** - Open new session with superpowers:executing-plans, batch execution with checkpoints

**Which approach would you prefer?**
